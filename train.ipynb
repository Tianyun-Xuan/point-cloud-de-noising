{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from src.pcd_de_noising import MistNet\n",
    "from src.pcd_de_noising import PCDDataset, PointCloudDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_GPUS = 1\n",
    "DATASET_PATH = \"./rayz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorboard logger\n",
    "logger = TensorBoardLogger(\"./log\", name=\"MistNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MistNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MistNet.load_from_checkpoint(\"log/MistNet/version_2/checkpoints/epoch=49-step=5650.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = PointCloudDataModule(\n",
    "    os.path.join(DATASET_PATH, \"train\"),\n",
    "    os.path.join(DATASET_PATH, \"val\"),\n",
    "    os.path.join(DATASET_PATH, \"test\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    overfit_batches=51,\n",
    "    max_epochs=50,\n",
    "    log_every_n_steps=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type                       | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | lila1             | LiLaBlock                  | 55.1 K\n",
      "1 | lila2             | LiLaBlock                  | 804 K \n",
      "2 | lila3             | LiLaBlock                  | 2.2 M \n",
      "3 | lila4             | LiLaBlock                  | 4.2 M \n",
      "4 | dropout           | Dropout2d                  | 0     \n",
      "5 | lila5             | LiLaBlock                  | 2.0 M \n",
      "6 | classifier        | Conv2d                     | 387   \n",
      "7 | accuracy          | MulticlassAccuracy         | 0     \n",
      "8 | average_precision | MulticlassAveragePrecision | 0     \n",
      "-----------------------------------------------------------------\n",
      "9.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.3 M     Total params\n",
      "37.281    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]Val found 51 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xavier/micromamba/envs/pcd-de-noising/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train found 51 files                                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xavier/micromamba/envs/pcd-de-noising/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:268: You requested to overfit but enabled train dataloader shuffling. We are turning off the train dataloader shuffling for you.\n",
      "/Users/xavier/micromamba/envs/pcd-de-noising/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 13/13 [00:55<00:00,  0.23it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 13/13 [00:55<00:00,  0.23it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    max_epochs=50,\n",
    "    log_every_n_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type                       | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | lila1             | LiLaBlock                  | 72.4 K\n",
      "1 | lila2             | LiLaBlock                  | 804 K \n",
      "2 | lila3             | LiLaBlock                  | 2.2 M \n",
      "3 | lila4             | LiLaBlock                  | 4.2 M \n",
      "4 | dropout           | Dropout2d                  | 0     \n",
      "5 | lila5             | LiLaBlock                  | 2.0 M \n",
      "6 | classifier        | Conv2d                     | 1.0 K \n",
      "7 | accuracy          | MulticlassAccuracy         | 0     \n",
      "8 | average_precision | MulticlassAveragePrecision | 0     \n",
      "-----------------------------------------------------------------\n",
      "9.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.3 M     Total params\n",
      "37.352    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]Val found 450 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xavier/micromamba/envs/pcd-de-noising/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train found 450 files                                                      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xavier/micromamba/envs/pcd-de-noising/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 113/113 [16:19<00:00,  0.12it/s, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 113/113 [16:19<00:00,  0.12it/s, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtest(model, datamodule\u001b[38;5;241m=\u001b[39mdata_module)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = data_module.test_dataloader()\n",
    "start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "reps = 100\n",
    "timings = [0] * reps\n",
    "\n",
    "distance, reflectivity, labels = next(iter(loader))\n",
    "distance = distance.cuda()\n",
    "reflectivity = reflectivity.cuda()\n",
    "labels = labels.cuda()\n",
    "\n",
    "for rep in range(reps):\n",
    "    start.record()\n",
    "    _ = model(distance, reflectivity)\n",
    "    end.record()\n",
    "\n",
    "    # Wait for GPU sync\n",
    "    torch.cuda.synchronize()\n",
    "    curr_time = start.elapsed_time(end)\n",
    "    timings[rep] = curr_time / 1000  # ms to s\n",
    "\n",
    "images_processed = reps * loader.batch_size\n",
    "mean_time = sum(timings) / images_processed\n",
    "f\"Mean inference time: {mean_time:.2f}s, mean FPS: {1 / mean_time:.2f}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "a23a2d6909c68acaee8cdc174eaa8f4ab01509589aa59c1ab9b2bf57fe831546"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
